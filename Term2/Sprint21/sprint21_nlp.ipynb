{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9ngGPwGr22g"
   },
   "source": [
    "# Sprint 自然言語処理入門"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7bF6xsFwr22i",
    "outputId": "5996f11e-510f-47af-9cc4-83bda5e1ab26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-07-29 08:28:23--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80.23M  22.8MB/s    in 5.2s    \n",
      "\n",
      "2020-07-29 08:28:29 (15.3 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IMDBをカレントフォルダにダウンロード\n",
    "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJKshP52r22o"
   },
   "outputs": [],
   "source": [
    "# 解凍\n",
    "!tar zxf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JipWXRFYr22s"
   },
   "outputs": [],
   "source": [
    "# aclImdb/train/unsupはラベル無しのため削除\n",
    "!rm -rf aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ovys40gPr22v",
    "outputId": "2228d1a8-f77e-4223-f694-44b129659bcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Movie Review Dataset v1.0\n",
      "\n",
      "Overview\n",
      "\n",
      "This dataset contains movie reviews along with their associated binary\n",
      "sentiment polarity labels. It is intended to serve as a benchmark for\n",
      "sentiment classification. This document outlines how the dataset was\n",
      "gathered, and how to use the files provided. \n",
      "\n",
      "Dataset \n",
      "\n",
      "The core dataset contains 50,000 reviews split evenly into 25k train\n",
      "and 25k test sets. The overall distribution of labels is balanced (25k\n",
      "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
      "documents for unsupervised learning. \n",
      "\n",
      "In the entire collection, no more than 30 reviews are allowed for any\n",
      "given movie because reviews for the same movie tend to have correlated\n",
      "ratings. Further, the train and test sets contain a disjoint set of\n",
      "movies, so no significant performance is obtained by memorizing\n",
      "movie-unique terms and their associated with observed labels.  In the\n",
      "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
      "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
      "more neutral ratings are not included in the train/test sets. In the\n",
      "unsupervised set, reviews of any rating are included and there are an\n",
      "even number of reviews > 5 and <= 5.\n",
      "\n",
      "Files\n",
      "\n",
      "There are two top-level directories [train/, test/] corresponding to\n",
      "the training and test sets. Each contains [pos/, neg/] directories for\n",
      "the reviews with binary labels positive and negative. Within these\n",
      "directories, reviews are stored in text files named following the\n",
      "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
      "the star rating for that review on a 1-10 scale. For example, the file\n",
      "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
      "example with unique id 200 and star rating 8/10 from IMDb. The\n",
      "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
      "omitted for this portion of the dataset.\n",
      "\n",
      "We also include the IMDb URLs for each review in a separate\n",
      "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
      "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
      "are unable to link directly to the review, but only to the movie's\n",
      "review page.\n",
      "\n",
      "In addition to the review text files, we include already-tokenized bag\n",
      "of words (BoW) features that were used in our experiments. These \n",
      "are stored in .feat files in the train/test directories. Each .feat\n",
      "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
      "data.  The feature indices in these files start from 0, and the text\n",
      "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
      "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
      "(the) appears 7 times in that review.\n",
      "\n",
      "LIBSVM page for details on .feat file format:\n",
      "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
      "\n",
      "We also include [imdbEr.txt] which contains the expected rating for\n",
      "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
      "rating is a good way to get a sense for the average polarity of a word\n",
      "in the dataset.\n",
      "\n",
      "Citing the dataset\n",
      "\n",
      "When using this dataset please cite our ACL 2011 paper which\n",
      "introduces it. This paper also contains classification results which\n",
      "you may want to compare against.\n",
      "\n",
      "\n",
      "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "  month     = {June},\n",
      "  year      = {2011},\n",
      "  address   = {Portland, Oregon, USA},\n",
      "  publisher = {Association for Computational Linguistics},\n",
      "  pages     = {142--150},\n",
      "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "}\n",
      "\n",
      "References\n",
      "\n",
      "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
      "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
      "636-659.\n",
      "\n",
      "Contact\n",
      "\n",
      "For questions/comments/corrections please contact Andrew Maas\n",
      "amaas@cs.stanford.edu\n"
     ]
    }
   ],
   "source": [
    "# IMDBデータセットの説明を表示\n",
    "!cat aclImdb/README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VRapoXr7r220"
   },
   "source": [
    "### データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QmALnlaur220"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hSUVgNGyr224",
    "outputId": "28533ffe-b8a9-4e0f-f24c-01e570425131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
    "x_train, y_train = train_review.data, train_review.target\n",
    "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
    "x_test, y_test = test_review.data, test_review.target\n",
    "# ラベルの0,1 と意味の対応の表示\n",
    "print(train_review.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "9-pEpsZ6r229",
    "outputId": "b31d0229-cf63-443c-b117-b897507f3046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x :Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n"
     ]
    }
   ],
   "source": [
    "# 中身の確認\n",
    "print(\"x :{}\".format(x_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d9MGb-nRsOHV"
   },
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is very good.\",\n",
    "  \"This film is a good\",\n",
    "  \"Very bad. Very, very bad.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "UC4QXPDpr23E",
    "outputId": "c862d763-48f6-4084-ca5d-85ace705e139"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>bad</th>\n",
       "      <th>film</th>\n",
       "      <th>good</th>\n",
       "      <th>is</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "      <th>very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  bad  film  good  is  movie  this  very\n",
       "0  0    0     0     1   1      1     1     1\n",
       "1  1    0     1     1   1      0     1     0\n",
       "2  0    2     0     0   0      0     0     3"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "\n",
    "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "H2ECwYMXsTNU",
    "outputId": "69015442-a501-42fc-bf48-0d6f80b02d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a good', 'bad very', 'film is', 'is a', 'is very', 'movie is', 'this film', 'this movie', 'very bad', 'very good', 'very very']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a good</th>\n",
       "      <th>bad very</th>\n",
       "      <th>film is</th>\n",
       "      <th>is a</th>\n",
       "      <th>is very</th>\n",
       "      <th>movie is</th>\n",
       "      <th>this film</th>\n",
       "      <th>this movie</th>\n",
       "      <th>very bad</th>\n",
       "      <th>very good</th>\n",
       "      <th>very very</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a good  bad very  film is  is a  ...  this movie  very bad  very good  very very\n",
       "0       0         0        0     0  ...           1         0          1          0\n",
       "1       1         0        1     1  ...           0         0          0          0\n",
       "2       0         1        0     0  ...           0         2          0          1\n",
       "\n",
       "[3 rows x 11 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "print(vectorizer.get_feature_names())\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYubpLMzr23I"
   },
   "source": [
    "## 【問題1】BoWのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhXqWsYPr23J"
   },
   "source": [
    "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n",
    "\n",
    ">This movie is SOOOO funny!!!  \n",
    "What a movie! I never  \n",
    "best movie ever!!!!! this movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gCXyqnfVr23J"
   },
   "outputs": [],
   "source": [
    "mini_dataset = \\\n",
    "  [\"This movie is SOOOO funny!!!\",\n",
    "  \"What a movie! I never\",\n",
    "  \"best movie ever!!!!! this movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlqNi2pUr23O"
   },
   "outputs": [],
   "source": [
    "class BoW_1_gram():\n",
    "  def __init__(self, token_pattern=r'(?u)\\b\\w+\\b'):\n",
    "    self.token_pattern = token_pattern\n",
    "  \n",
    "  def fit(self, sentence):\n",
    "    \"\"\"\n",
    "    文を引数として受け取り、コーパスをndarray型で返す\n",
    "    \"\"\"\n",
    "    sentence_num = len(sentence)\n",
    "    \n",
    "    #正規表現で分割\n",
    "    splits = []\n",
    "    for s in sentence:\n",
    "      splits.append(re.findall(self.token_pattern, s))\n",
    "    \n",
    "    #小文字化\n",
    "    lower = []\n",
    "    for l in splits:\n",
    "      lower.append([str.lower() for str in l])\n",
    "\n",
    "    #flattenして重複排除\n",
    "    labels = list(set(sum(lower, [])))\n",
    "\n",
    "    copus = np.empty((sentence_num,len(labels)))\n",
    "    for i ,label in enumerate(labels):\n",
    "      for row, value in enumerate(lower):\n",
    "        copus[row, i] =  value.count(label)\n",
    "    \n",
    "    self.labels = labels\n",
    "\n",
    "    return copus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8qO9LucXr23R",
    "outputId": "99892734-d7c0-44d2-d0e6-7ca8dc37f035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 2. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "gram = BoW_1_gram()\n",
    "result = gram.fit(mini_dataset)\n",
    "\n",
    "# print(gram.labels)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ni4yK8tkr23V"
   },
   "outputs": [],
   "source": [
    "class BoW_2_gram(BoW_1_gram):\n",
    "    def fit(self, sentence):\n",
    "        \n",
    "      sentence_num = len(sentence)\n",
    "\n",
    "      splits = []\n",
    "      for s in sentence:\n",
    "        splits.append(re.findall(self.token_pattern, s))\n",
    "      \n",
    "      lower = []\n",
    "      for l in splits:\n",
    "        lower.append([str.lower() for str in l])\n",
    "\n",
    "      two_words = []\n",
    "      for s in lower:\n",
    "        temp = []\n",
    "        for i in range(len(s)-1):\n",
    "          temp.append(s[i]+\" \"+s[i+1])\n",
    "        two_words.append(temp)\n",
    "\n",
    "      labels = list(set(sum(two_words, [])))\n",
    "\n",
    "      copus = np.empty((sentence_num,len(labels)))\n",
    "      for i ,label in enumerate(labels):\n",
    "        for row, value in enumerate(two_words):\n",
    "          copus[row, i] =  value.count(label)\n",
    "      \n",
    "      self.labels = labels\n",
    "\n",
    "      return copus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "iNVHf4o5r23Y",
    "outputId": "d8d4063e-bd96-47ac-f401-71912f959593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1.]\n",
      " [0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "gram2 = BoW_2_gram()\n",
    "result2 = gram2.fit(mini_dataset)\n",
    "\n",
    "#print(gram2.labels)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzV4WCuqr23b"
   },
   "source": [
    "## 【問題2】TF-IDFの計算\n",
    "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "ufAB4Oa1r23f",
    "outputId": "05761eb9-68fb-4d14-d014-2692c019c374"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>funny</th>\n",
       "      <th>i</th>\n",
       "      <th>movie</th>\n",
       "      <th>never</th>\n",
       "      <th>soooo</th>\n",
       "      <th>this</th>\n",
       "      <th>what</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  best  ever  funny  i  movie  never  soooo  this  what\n",
       "0  0     0     0      1  0      1      0      1     1     0\n",
       "1  1     0     0      0  1      1      1      0     0     1\n",
       "2  0     1     1      0  0      2      0      0     1     0"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "q_5m2kxer23b",
    "outputId": "083e0dcb-b5f6-4c4b-9ad3-3e1a567b47fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# はじめて使う場合はストップワードをダウンロード\n",
    "import nltk\n",
    "stop_words = nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stop_words = stopwords.words('english')\n",
    "print(\"stop word : {}\".format(nltk_stop_words)) # 'i', 'me', 'my', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "colab_type": "code",
    "id": "nAupWT0Sr23j",
    "outputId": "2fab3eee-b039-4938-9fe1-b8dc3563ea08"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>best</th>\n",
       "      <th>ever</th>\n",
       "      <th>movie</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  best  ever  movie  this\n",
       "0  0     0     0      1     1\n",
       "1  1     0     0      1     0\n",
       "2  0     1     1      2     1"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
    "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
    "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zgco2vSQtCvi",
    "outputId": "39b1d3ec-3a9f-48a4-b9ba-c41b043a3701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=nltk_stop_words, token_pattern=r'\\b\\w+\\b', max_features = 5000)\n",
    "bow_train = (vectorizer.fit_transform(x_train)).toarray()\n",
    "print(bow_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIOO79Pjr23p"
   },
   "source": [
    "# 【問題3】TF-IDFを用いた学習\n",
    "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n",
    "\n",
    "\n",
    "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QlqpRmier23q",
    "outputId": "534481b8-fe77-4944-dbab-005d94b582c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow と tf.keras のインポート\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvJnxdEat7Gy"
   },
   "source": [
    "#### NNモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "UmvOXQRdta3c",
    "outputId": "ee03dc26-a95d-4039-8289-a32d23e6c0ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10000)             50010000  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6000)              60006000  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3000)              18003000  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              3001000   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               100100    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 131,120,302\n",
      "Trainable params: 131,120,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10000, activation='relu', input_shape=(5000,)),\n",
    "    keras.layers.Dense(6000, activation='relu'),\n",
    "    keras.layers.Dense(3000, activation='relu'),\n",
    "    keras.layers.Dense(1000, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "nJBmQi69ta0I",
    "outputId": "cb558df5-df7a-494c-ee47-34dbd5e5ff93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 741s 947ms/step - loss: 0.3731 - accuracy: 0.8582\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 737s 942ms/step - loss: 0.1813 - accuracy: 0.9326\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 742s 949ms/step - loss: 0.0659 - accuracy: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f08171e0ba8>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(bow_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "__nubaRktm4W",
    "outputId": "05ee58be-fa75-4224-dcba-f927b197ee63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.489439994096756\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(stop_words=nltk_stop_words, token_pattern=r'\\b\\w+\\b', max_features = 5000)\n",
    "bow_test = (vectorizer2.fit_transform(x_test)).toarray()\n",
    "loss, accuracy = model.evaluate(bow_test, y_test, verbose=0)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "81HmQ_oztmzs",
    "outputId": "bb17ea2d-51b1-4b96-fa92-dd850ccf2751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8624799847602844\n"
     ]
    }
   ],
   "source": [
    "bow_test_same = (vectorizer.transform(x_test)).toarray()\n",
    "loss2, accuracy2 = model.evaluate(bow_test_same, y_test, verbose=0)\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "juo9lJpguDge"
   },
   "source": [
    "#### TF-IDFモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twlg060duIHU"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_tf = TfidfVectorizer(stop_words=nltk_stop_words, token_pattern=r'\\b\\w+\\b', max_features = 5000)\n",
    "bow_train_tf = (vectorizer_tf.fit_transform(x_train)).toarray()\n",
    "test_tf = (vectorizer_tf.transform(x_test)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "bi4njmdtuIEk",
    "outputId": "f79e4982-440e-4e44-b74e-b79a0b9253dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 739s 944ms/step - loss: 0.3461 - accuracy: 0.8465\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 738s 943ms/step - loss: 0.1688 - accuracy: 0.9358\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 737s 942ms/step - loss: 0.0330 - accuracy: 0.9904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0813724b38>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#K.clear_session()\n",
    "\n",
    "model2 = keras.Sequential([\n",
    "    keras.layers.Dense(10000, activation='relu', input_shape=(5000,)),\n",
    "    keras.layers.Dense(6000, activation='relu'),\n",
    "    keras.layers.Dense(3000, activation='relu'),\n",
    "    keras.layers.Dense(1000, activation='relu'),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.fit(bow_train_tf, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d-WXxHd7uOn3",
    "outputId": "ff7ffbd9-b529-4c2b-f4ac-c869b49ef182"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8669599890708923\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model2.evaluate(test_tf, y_test, verbose=0)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7etiVDF0r23t"
   },
   "source": [
    "## 【問題4】TF-IDFのスクラッチ実装 \n",
    "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQ1xuqHoBLFi"
   },
   "outputs": [],
   "source": [
    "test_dataset = [\n",
    "                \"This movie is SOOOO funny!!!\", \n",
    "                \"What a movie! I never\", \n",
    "                \"best movie ever!!!!! this movie\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwqYkxw9uhe6"
   },
   "source": [
    "#### 標準"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTc-Oafor23u"
   },
   "outputs": [],
   "source": [
    "class TFIDF_1_gram():\n",
    "  def __init__(self, token_pattern=r'(?u)\\b\\w+\\b'):\n",
    "    self.token_pattern = token_pattern\n",
    "  \n",
    "  def fit(self, sentence):\n",
    "    sentence_num = len(sentence)\n",
    "    \n",
    "    splits = []\n",
    "    for s in sentence:\n",
    "      splits.append(re.findall(self.token_pattern, s))\n",
    "    \n",
    "    lower = []\n",
    "    for l in splits:\n",
    "      lower.append([str.lower() for str in l])\n",
    "\n",
    "    labels = list(set(sum(lower, [])))\n",
    "\n",
    "    copus = np.empty((sentence_num,len(labels)))\n",
    "    for i ,label in enumerate(labels):\n",
    "      for row, value in enumerate(lower):\n",
    "        copus[row, i] =  value.count(label)\n",
    "    \n",
    "    self.labels = labels\n",
    "    self.copus = copus\n",
    "\n",
    "    self.tf = np.zeros(copus.shape)\n",
    "    for i, c in enumerate(list(copus)):\n",
    "      for j, value in enumerate(c):\n",
    "        self.tf[i, j] = value/sum(c)\n",
    "  \n",
    "    self.idf = np.log(sentence_num / np.count_nonzero(copus>0, axis=0))\n",
    "    \n",
    "    self.tfidf = self.tf * self.idf\n",
    "\n",
    "    return copus * self.tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "CnPYxKUMr23z",
    "outputId": "55b116ec-a722-4f45-816c-3ecaf5203e2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copus\n",
      " [[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 2. 1. 1. 0.]]\n",
      "tf\n",
      " [[0.2 0.  0.2 0.2 0.2 0.  0.  0.2 0.  0.  0. ]\n",
      " [0.  0.2 0.  0.  0.  0.2 0.2 0.2 0.  0.  0.2]\n",
      " [0.  0.  0.  0.  0.2 0.  0.  0.4 0.2 0.2 0. ]]\n",
      "idf\n",
      " [1.09861229 1.09861229 1.09861229 1.09861229 0.40546511 1.09861229\n",
      " 1.09861229 0.         1.09861229 1.09861229 1.09861229]\n",
      "tfidf\n",
      " [[0.21972246 0.         0.21972246 0.21972246 0.08109302 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.21972246 0.         0.         0.         0.21972246\n",
      "  0.21972246 0.         0.         0.         0.21972246]\n",
      " [0.         0.         0.         0.         0.08109302 0.\n",
      "  0.         0.         0.21972246 0.21972246 0.        ]]\n",
      "result\n",
      " [[0.21972246 0.         0.21972246 0.21972246 0.08109302 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.21972246 0.         0.         0.         0.21972246\n",
      "  0.21972246 0.         0.         0.         0.21972246]\n",
      " [0.         0.         0.         0.         0.08109302 0.\n",
      "  0.         0.         0.21972246 0.21972246 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TFIDF_1_gram()\n",
    "result = tfidf.fit(test_dataset)\n",
    "\n",
    "print(\"copus\\n\", tfidf.copus)\n",
    "print(\"tf\\n\",tfidf.tf)\n",
    "print(\"idf\\n\",tfidf.idf)\n",
    "print(\"tfidf\\n\", tfidf.tfidf )\n",
    "print(\"result\\n\" ,result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXLQzbAxuv6x"
   },
   "source": [
    "#### sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GeQ_Ell-r236"
   },
   "outputs": [],
   "source": [
    "class TFIDF_1_gram_SK():\n",
    "  def __init__(self, token_pattern=r'(?u)\\b\\w+\\b'):\n",
    "    self.token_pattern = token_pattern\n",
    "  \n",
    "  def fit(self, sentence):\n",
    "    sentence_num = len(sentence)\n",
    "    \n",
    "    splits = []\n",
    "    for s in sentence:\n",
    "      splits.append(re.findall(self.token_pattern, s))\n",
    "    \n",
    "    lower = []\n",
    "    for l in splits:\n",
    "      lower.append([str.lower() for str in l])\n",
    "\n",
    "    labels = list(set(sum(lower, [])))\n",
    "\n",
    "    copus = np.empty((sentence_num,len(labels)))\n",
    "    for i ,label in enumerate(labels):\n",
    "      for row, value in enumerate(lower):\n",
    "        copus[row, i] =  value.count(label)\n",
    "    \n",
    "    self.labels = labels\n",
    "    self.copus = copus\n",
    "\n",
    "    #tf計算\n",
    "    self.tf = copus\n",
    "  \n",
    "    self.idf = np.log(sentence_num + 1 / np.count_nonzero(copus>0, axis=0) + 1) + 1\n",
    "    \n",
    "    self.tfidf = self.tf * self.idf\n",
    "\n",
    "    return copus * self.tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "37kXqGnqr239",
    "outputId": "72c3caa2-a853-4212-ad55-3ff90257a3ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copus\n",
      " [[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 2. 1. 1. 0.]]\n",
      "tf\n",
      " [[1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 2. 1. 1. 0.]]\n",
      "idf\n",
      " [2.60943791 2.60943791 2.60943791 2.60943791 2.5040774  2.60943791\n",
      " 2.60943791 2.46633707 2.60943791 2.60943791 2.60943791]\n",
      "tfidf\n",
      " [[2.60943791 0.         2.60943791 2.60943791 2.5040774  0.\n",
      "  0.         2.46633707 0.         0.         0.        ]\n",
      " [0.         2.60943791 0.         0.         0.         2.60943791\n",
      "  2.60943791 2.46633707 0.         0.         2.60943791]\n",
      " [0.         0.         0.         0.         2.5040774  0.\n",
      "  0.         4.93267414 2.60943791 2.60943791 0.        ]]\n",
      "result\n",
      " [[2.60943791 0.         2.60943791 2.60943791 2.5040774  0.\n",
      "  0.         2.46633707 0.         0.         0.        ]\n",
      " [0.         2.60943791 0.         0.         0.         2.60943791\n",
      "  2.60943791 2.46633707 0.         0.         2.60943791]\n",
      " [0.         0.         0.         0.         2.5040774  0.\n",
      "  0.         9.86534828 2.60943791 2.60943791 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfidf2 = TFIDF_1_gram_SK()\n",
    "result2 = tfidf2.fit(test_dataset)\n",
    "\n",
    "print(\"copus\\n\", tfidf2.copus)\n",
    "print(\"tf\\n\",tfidf2.tf)\n",
    "print(\"idf\\n\",tfidf2.idf)\n",
    "print(\"tfidf\\n\", tfidf2.tfidf )\n",
    "print(\"result\\n\" ,result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEeLz3RuvgVv"
   },
   "source": [
    "## 【問題5】コーパスの前処理\n",
    "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eOqX7Ryor24B",
    "outputId": "783649bc-a5ed-4d56-ffbd-513911c7bf49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "splits = []\n",
    "for s in x_test:\n",
    "  splits.append(re.findall(r'(?u)\\b\\w+\\b', s))\n",
    "\n",
    "lower = []\n",
    "for l in splits:\n",
    "  lower.append([str.lower() for str in l])\n",
    "\n",
    "print(len(lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vRLcpR1SvsyT"
   },
   "source": [
    "## 【問題6】Word2Vecの学習\n",
    "Word2Vecの学習を行なってください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "_qG578YCBc7E",
    "outputId": "d0384fb8-beb7-4feb-bc64-523958c8c9a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
      "thisのベクトル : \n",
      "[ 0.0249938  -0.03765341  0.0195027   0.00524712 -0.00153644 -0.03273581\n",
      "  0.02355515 -0.01225791  0.01114472 -0.00907992]\n",
      "movieのベクトル : \n",
      "[ 0.00848707  0.02872956 -0.0244792  -0.03368572  0.0108247   0.04854515\n",
      "  0.00619196 -0.02328659 -0.03074072  0.03229745]\n",
      "isのベクトル : \n",
      "[-0.00286664 -0.00359677  0.00749168  0.0314682   0.01639835 -0.01668038\n",
      "  0.00981281 -0.04795818  0.01957807 -0.00324648]\n",
      "veryのベクトル : \n",
      "[-0.02707146  0.0428793   0.04621729 -0.04383265  0.0194873  -0.02036348\n",
      " -0.03272589 -0.04095151 -0.03248588 -0.00572642]\n",
      "goodのベクトル : \n",
      "[-0.00238406  0.01335714  0.02903937 -0.00371578 -0.0458021   0.04129339\n",
      "  0.02749515 -0.00501445 -0.03773468 -0.03934373]\n",
      "filmのベクトル : \n",
      "[ 0.02758595  0.00256787 -0.02125228  0.00187987  0.01038363  0.01999831\n",
      " -0.04292028  0.01096623 -0.00348892  0.00390196]\n",
      "aのベクトル : \n",
      "[ 0.00601908 -0.00562103  0.0379864  -0.00409364  0.03662813  0.00152588\n",
      "  0.01260309 -0.03621463 -0.01831621 -0.01350474]\n",
      "badのベクトル : \n",
      "[-0.00025829  0.02195159  0.02659402 -0.02034647  0.02813877  0.03181091\n",
      " -0.03445288 -0.04482948 -0.01158502 -0.04421905]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
    "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
    "model.build_vocab(sentences) # 準備\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
    "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
    "for vocab in model.wv.vocab.keys():\n",
    "  print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "mqgewnlNvtc3",
    "outputId": "be2b621a-1fb0-4f32-ecf5-2c4ceaefed2d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(22107793, 29600110)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(min_count=1, size=10) \n",
    "model.build_vocab(lower) \n",
    "model.train(lower, total_examples=model.corpus_count, epochs=model.iter) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Sprint21_nlp-Copy2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
