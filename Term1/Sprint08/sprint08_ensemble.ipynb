{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint アンサンブル学習 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】ブレンディングのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.read_csv(\"dataset/train.csv\")\n",
    "df = df_original[[\"GrLivArea\", \"YearBuilt\", \"SalePrice\"]]\n",
    "\n",
    "# 説明変数と目的変数に分割し\n",
    "X = df[[\"GrLivArea\", \"YearBuilt\"]].values\n",
    "y = df[[\"SalePrice\"]].values\n",
    "\n",
    "# トレーニングデータとテストデータに分割\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 目的変数を対数変換\n",
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)\n",
    "\n",
    "# 1次元に変更\n",
    "y_train=np.reshape(y_train,(-1))\n",
    "y_test=np.reshape(y_test,(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05186 [LinearRegression]\n",
      "0.04744 [SVR]\n",
      "0.06857 [DecisionTreeRegressor]\n",
      "0.05114 [RandomForestRegressor]\n",
      "0.04718 [LGBMRegressor]\n"
     ]
    }
   ],
   "source": [
    "# 単一モデルの作成\n",
    "lnr = LinearRegression()\n",
    "svr = SVR(gamma='auto')\n",
    "dtr = DecisionTreeRegressor(random_state=0)\n",
    "rf = RandomForestRegressor(random_state=0)\n",
    "lgb = LGBMRegressor(random_state=0)\n",
    "\n",
    "# 単一モデルごとに学習\n",
    "lnr.fit(X_train,y_train)\n",
    "svr.fit(X_train,y_train)\n",
    "dtr.fit(X_train,y_train)\n",
    "rf.fit(X_train,y_train)\n",
    "lgb.fit(X_train,y_train)\n",
    "\n",
    "# 単一モデルの評価\n",
    "print('{:.5f} [LinearRegression]'.format(mean_squared_error(y_test,lnr.predict(X_test))))\n",
    "print('{:.5f} [SVR]'.format(mean_squared_error(y_test,svr.predict(X_test))))\n",
    "print('{:.5f} [DecisionTreeRegressor]'.format(mean_squared_error(y_test,dtr.predict(X_test))))\n",
    "print('{:.5f} [RandomForestRegressor]'.format(mean_squared_error(y_test,rf.predict(X_test))))\n",
    "print('{:.5f} [LGBMRegressor]'.format(mean_squared_error(y_test,lgb.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単一モデルでは SVR と LGBMRegressor の数値が比較的良好でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つのモデルをブレンドする\n",
    "def blending(X_train, X_test, y_train, model1, model2):\n",
    "    # モデル1の学習\n",
    "    model1.fit(X_train, y_train)\n",
    "    # モデル1で予測\n",
    "    model1_pred = model1.predict(X_test)\n",
    "    # モデル2の学習\n",
    "    model2.fit(X_train, y_train)\n",
    "    # モデル2で予測\n",
    "    model2_pred = model2.predict(X_test)\n",
    "    # モデル1と2の平均を出力\n",
    "    blend_pred = (model1_pred + model2_pred) / 2\n",
    "    # 評価\n",
    "    mse_result = mean_squared_error(y_test,blend_pred)\n",
    "    # r2_score_result = r2_score(y_test,blend_pred)\n",
    "    \n",
    "    return mse_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04855 [lnr x svr]\n",
      "0.04936 [lnr x dtr]\n",
      "0.04641 [lnr x rf]\n",
      "0.04656 [lnr x lgb]\n",
      "0.04767 [svr x dtr]\n",
      "0.04469 [svr x rf]\n",
      "0.04513 [svr x lgb]\n",
      "0.05363 [dtr x rf]\n",
      "0.04931 [dtr x lgb]\n",
      "0.04683 [rf x lgb]\n"
     ]
    }
   ],
   "source": [
    "#blending(X_train, X_test, y_train, lnr, lgb)\n",
    "print('{:.5f} [lnr x svr]'.format(blending(X_train, X_test, y_train, lnr, svr)))\n",
    "print('{:.5f} [lnr x dtr]'.format(blending(X_train, X_test, y_train, lnr, dtr)))\n",
    "print('{:.5f} [lnr x rf]'.format(blending(X_train, X_test, y_train, lnr, rf)))\n",
    "print('{:.5f} [lnr x lgb]'.format(blending(X_train, X_test, y_train, lnr, lgb)))\n",
    "print('{:.5f} [svr x dtr]'.format(blending(X_train, X_test, y_train, svr, dtr)))\n",
    "print('{:.5f} [svr x rf]'.format(blending(X_train, X_test, y_train, svr, rf)))\n",
    "print('{:.5f} [svr x lgb]'.format(blending(X_train, X_test, y_train, svr, lgb)))\n",
    "print('{:.5f} [dtr x rf]'.format(blending(X_train, X_test, y_train, dtr, rf)))\n",
    "print('{:.5f} [dtr x lgb]'.format(blending(X_train, X_test, y_train, dtr, lgb)))\n",
    "print('{:.5f} [rf x lgb]'.format(blending(X_train, X_test, y_train, rf, lgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つのモデルをブレンドした結果、[SVR x RandomForest]と[SVR x LightGBM]の組み合わせが良好な結果となりました。\n",
    "なお、単一モデルよりも精度が良かった組み合わせは下記となります。\n",
    "- [LinearRegression x RandomForestRegressor]\n",
    "- [LinearRegression x LGBMRegressor]\n",
    "- [SVR x RandomForestRegressor]\n",
    "- [SVR x LGBMRegressor]\n",
    "- [RandomForestRegressor x LGBMRegressor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】バギングのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.empty((X_test.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagging_tree = np.zeros(len(y_test)).reshape(-1, 1)\n",
    "bagging_tree.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "指定した回数のループを実行:\n",
    "    指定した行数に分割して抽出\n",
    "    抽出したデータに対して学習を行う\n",
    "    予測した結果をリストに蓄積する\n",
    "\"\"\" \n",
    "    \n",
    "def Bagging(X_train, y_train, X_test, model, cnt=5, split=0.2):\n",
    "    # リストの初期化\n",
    "    y_pred = np.zeros((X_test.shape[0], 2))\n",
    "    y_pred[:,0:2] = X_test\n",
    "    # 指定回数のループ\n",
    "    for i in range(cnt):\n",
    "        (X_train_bg, _, y_train_bg, _) = train_test_split(X_train, y_train, test_size=split, shuffle=True)            \n",
    "        # 学習\n",
    "        model.fit(X_train_bg, y_train_bg)\n",
    "        # 予測\n",
    "        model_pred = model.predict(X_test)\n",
    "        # 予想結果の蓄積\n",
    "        y_pred = np.c_[y_pred, model_pred]\n",
    "            \n",
    "    #平均の計算\n",
    "    y_pred_mean = np.mean(y_pred[:,2:cnt+2], axis=1)\n",
    "    # 平均も結合\n",
    "    y_pred = np.c_[y_pred, y_pred_mean]\n",
    "    \n",
    "    return y_pred[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05169 [LinearRegression]\n",
      "0.04748 [SVR]\n",
      "0.05410 [DecisionTreeRegressor]\n",
      "0.04582 [RandomForestRegressor]\n",
      "0.04785 [LGBMRegressor]\n"
     ]
    }
   ],
   "source": [
    "# 単一モデル&バギング実施の評価\n",
    "print('{:.5f} [LinearRegression]'.format(mean_squared_error(y_test,Bagging(X_train, y_train, X_test, lnr))))\n",
    "print('{:.5f} [SVR]'.format(mean_squared_error(y_test,Bagging(X_train, y_train, X_test, svr))))\n",
    "print('{:.5f} [DecisionTreeRegressor]'.format(mean_squared_error(y_test,Bagging(X_train, y_train, X_test, dtr))))\n",
    "print('{:.5f} [RandomForestRegressor]'.format(mean_squared_error(y_test,Bagging(X_train, y_train, X_test, rf))))\n",
    "print('{:.5f} [LGBMRegressor]'.format(mean_squared_error(y_test,Bagging(X_train, y_train, X_test, lgb))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM以外のモデルに関しては、バギングを行うことでパフォーマンスの向上が見られました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】スタッキングのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stacking():\n",
    "    def __init__(self, split_n=3, model_n=2):\n",
    "        self.split_n = split_n\n",
    "        self.model_n = model_n\n",
    "\n",
    "    def fit(self, X_train, y_train, X_test, y_test, models):\n",
    "        # K個に分割するdividerを作る\n",
    "        divider = np.zeros(self.split_n)\n",
    "        vol = X_train.shape[0]\n",
    "        num = self.split_n\n",
    "        for i in range(self.split_n):\n",
    "            divider[i] = math.ceil(vol/num)\n",
    "            num -= 1\n",
    "            vol = vol-divider[i]\n",
    "        \n",
    "        self.divider = divider.astype(int)\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.models = models\n",
    "        # print(self.divider)\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        for m in range(self.model_n):\n",
    "            divide_point = 0\n",
    "            for n in range(self.split_n):\n",
    "                idx = np.zeros(X_train.shape[0], dtype=bool)\n",
    "                idx[divide_point:divide_point+self.divider[n]]= True\n",
    "                self.X_test_divided = X_train[idx, :]\n",
    "                self.X_train_divided = X_train[~idx, :]\n",
    "                self.y_test_divided = y_train[idx]\n",
    "                self.y_train_divided = y_train[~idx]                    \n",
    "                \n",
    "                models[m].fit(self.X_train_divided, self.y_train_divided)\n",
    "                if n == 0:\n",
    "                    blend = models[m].predict(self.X_test_divided)\n",
    "                    pred_data = models[m].predict(X_test)\n",
    "                else:\n",
    "                    blend = np.r_[blend, models[m].predict(self.X_test_divided)]\n",
    "                    pred_data = np.c_[pred_data, models[m].predict(X_test)]\n",
    "            \n",
    "                divide_point += self.divider[n]\n",
    "            if m ==0:\n",
    "                blend_data =blend.reshape(-1, 1)\n",
    "                blend_pred_data = np.mean(pred_data, axis=1)\n",
    "            else:\n",
    "                blend_data = np.c_[blend_data, blend.reshape(-1, 1) ]\n",
    "                blend_pred_data = np.c_[blend_pred_data, np.mean(pred_data, axis=1)]\n",
    "        \n",
    "        models[0].fit(blend_data, y_train)\n",
    "        y_pred = models[0].predict(blend_pred_data)                               \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04715 [lnr x svr]\n",
      "0.04884 [lnr x dtr]\n",
      "0.04530 [lnr x rf]\n",
      "0.04692 [lnr x lgb]\n",
      "0.04777 [svr x dtr]\n",
      "0.04548 [svr x rf]\n",
      "0.04667 [svr x lgb]\n",
      "0.10924 [dtr x rf]\n",
      "0.09536 [dtr x lgb]\n",
      "0.04867 [rf x lgb]\n"
     ]
    }
   ],
   "source": [
    "stacking = Stacking()\n",
    "models = [LinearRegression(), SVR(gamma='auto')]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [lnr x svr]\".format(mean_squared_error(y_test, stacking.predict(X_test))))\n",
    "\n",
    "models = [LinearRegression(), DecisionTreeRegressor(random_state=0)]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [lnr x dtr]\".format(mean_squared_error(y_test, stacking.predict(X_test))))\n",
    "\n",
    "models = [LinearRegression(), RandomForestRegressor(random_state=0)]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [lnr x rf]\".format(mean_squared_error(y_test, stacking.predict(X_test))))\n",
    "\n",
    "models = [LinearRegression(), LGBMRegressor(random_state=0)]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [lnr x lgb]\".format(mean_squared_error(y_test, stacking.predict(X_test))))\n",
    "\n",
    "models = [SVR(gamma='auto'), DecisionTreeRegressor(random_state=0)]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [svr x dtr]\".format(mean_squared_error(y_test, stacking.predict(X_test))))\n",
    "\n",
    "models = [SVR(gamma='auto'), RandomForestRegressor(random_state=0)]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [svr x rf]\".format(mean_squared_error(y_test, stacking.predict(X_test))))\n",
    "\n",
    "models = [SVR(gamma='auto'), LGBMRegressor(random_state=0)]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [svr x lgb]\".format(mean_squared_error(y_test, stacking.predict(X_test))))\n",
    "\n",
    "models = [DecisionTreeRegressor(random_state=0), RandomForestRegressor(random_state=0)]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [dtr x rf]\".format(mean_squared_error(y_test, stacking.predict(X_test))))\n",
    "\n",
    "models = [DecisionTreeRegressor(random_state=0), LGBMRegressor(random_state=0)]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [dtr x lgb]\".format(mean_squared_error(y_test, stacking.predict(X_test))))\n",
    "\n",
    "models = [RandomForestRegressor(random_state=0), LGBMRegressor(random_state=0)]\n",
    "stacking.fit(X_train, y_train, X_test, y_test, models)\n",
    "print(\"{:.5f} [rf x lgb]\".format(mean_squared_error(y_test, stacking.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つのモデルをスタッキングした結果、下記の組み合わせが単一モデルの数値（ [LGBMRegressor]の0.04718）よりも精度が良好な結果となりました。\n",
    "\n",
    "- [LinearRegression x SVR]\n",
    "- [LinearRegression x RandomForestRegressor]\n",
    "- [LinearRegression x LGBMRegressor]\n",
    "- [SVR x RandomForestRegressor]\n",
    "- [SVR x LGBMRegressor]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
